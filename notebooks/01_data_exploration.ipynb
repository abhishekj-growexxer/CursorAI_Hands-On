{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Store Sales Time Series Forecasting - Data Exploration\n",
        "\n",
        "This notebook explores the Favorita store sales dataset to understand:\n",
        "1. Data structure and quality\n",
        "2. Temporal patterns\n",
        "3. Store and product relationships\n",
        "4. External factors (oil prices, holidays)\n",
        "5. Feature relationships and correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn')\n",
        "sns.set_palette('husl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_dataset(df, name):\n",
        "    \"\"\"\n",
        "    Comprehensive analysis of a dataset including:\n",
        "    - Basic info (size, memory usage)\n",
        "    - Column details (types, unique values, missing values)\n",
        "    - Descriptive statistics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Analysis for: {name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Basic Information\n",
        "    print(\"\\n1. Basic Information:\")\n",
        "    print(f\"Number of Records: {len(df):,}\")\n",
        "    print(f\"Number of Columns: {len(df.columns):,}\")\n",
        "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / (1024*1024):.2f} MB\")\n",
        "    \n",
        "    # Column Analysis\n",
        "    print(\"\\n2. Column Details:\")\n",
        "    for col in df.columns:\n",
        "        print(f\"\\nColumn: {col}\")\n",
        "        print(f\"Type: {df[col].dtype}\")\n",
        "        print(f\"Unique Values: {df[col].nunique():,}\")\n",
        "        print(f\"Missing Values: {df[col].isnull().sum():,} ({(df[col].isnull().sum()/len(df))*100:.2f}%)\")\n",
        "        \n",
        "        # For numeric columns\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            print(f\"Min: {df[col].min():,}\")\n",
        "            print(f\"Max: {df[col].max():,}\")\n",
        "            print(f\"Mean: {df[col].mean():.2f}\")\n",
        "        \n",
        "        # For categorical/object columns\n",
        "        elif df[col].dtype == 'object':\n",
        "            if df[col].nunique() < 10:  # Only show if few unique values\n",
        "                print(\"Value Counts:\")\n",
        "                print(df[col].value_counts().head())\n",
        "    \n",
        "    # Memory Optimization Suggestions\n",
        "    print(\"\\n3. Memory Optimization Suggestions:\")\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'float64' and df[col].notnull().all():\n",
        "            if df[col].round(0).equals(df[col]):\n",
        "                print(f\"- Column '{col}' could be converted to integer type\")\n",
        "        elif df[col].dtype == 'object':\n",
        "            if df[col].nunique() / len(df) < 0.5:  # Less than 50% unique values\n",
        "                print(f\"- Column '{col}' could be converted to categorical type\")\n",
        "\n",
        "# Analyze each dataset\n",
        "analyze_dataset(train_df, \"Training Data\")\n",
        "analyze_dataset(stores_df, \"Stores Data\")\n",
        "analyze_dataset(oil_df, \"Oil Prices Data\")\n",
        "analyze_dataset(holidays_df, \"Holidays Data\")\n",
        "analyze_dataset(transactions_df, \"Transactions Data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Data Dictionary\n",
        "\n",
        "1. **Training Data (`train_df`)**:\n",
        "   - `date`: Date of the record\n",
        "   - `store_nbr`: Store identifier\n",
        "   - `family`: Product family (category)\n",
        "   - `sales`: Total sales for the product family at the store\n",
        "   - `onpromotion`: Number of items in the product family on promotion\n",
        "\n",
        "2. **Stores Data (`stores_df`)**:\n",
        "   - `store_nbr`: Store identifier\n",
        "   - `city`: City where the store is located\n",
        "   - `state`: State where the store is located\n",
        "   - `type`: Type of store\n",
        "   - `cluster`: Store cluster based on similarity\n",
        "\n",
        "3. **Oil Prices Data (`oil_df`)**:\n",
        "   - `date`: Date of the oil price record\n",
        "   - `dcoilwtico`: Daily price of oil in USD\n",
        "\n",
        "4. **Holidays Data (`holidays_df`)**:\n",
        "   - `date`: Date of the holiday/event\n",
        "   - `type`: Type of holiday (Holiday, Transfer, Additional, Bridge, Work Day)\n",
        "   - `locale`: National, Regional, or Local\n",
        "   - `locale_name`: Specific region if Regional/Local\n",
        "   - `description`: Description of the holiday\n",
        "   - `transferred`: Boolean indicating if the holiday was transferred\n",
        "\n",
        "5. **Transactions Data (`transactions_df`)**:\n",
        "   - `date`: Date of transactions\n",
        "   - `store_nbr`: Store identifier\n",
        "   - `transactions`: Number of transactions on the given date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Range Analysis\n",
        "print(\"Dataset Time Ranges:\")\n",
        "print(\"\\nTraining Data:\")\n",
        "print(f\"Start Date: {train_df['date'].min()}\")\n",
        "print(f\"End Date: {train_df['date'].max()}\")\n",
        "print(f\"Total Days: {(train_df['date'].max() - train_df['date'].min()).days}\")\n",
        "\n",
        "print(\"\\nOil Prices Data:\")\n",
        "print(f\"Start Date: {oil_df['date'].min()}\")\n",
        "print(f\"End Date: {oil_df['date'].max()}\")\n",
        "print(f\"Total Days: {(oil_df['date'].max() - oil_df['date'].min()).days}\")\n",
        "\n",
        "print(\"\\nTransactions Data:\")\n",
        "print(f\"Start Date: {transactions_df['date'].min()}\")\n",
        "print(f\"End Date: {transactions_df['date'].max()}\")\n",
        "print(f\"Total Days: {(transactions_df['date'].max() - transactions_df['date'].min()).days}\")\n",
        "\n",
        "# Check for data completeness\n",
        "print(\"\\nData Completeness Check:\")\n",
        "total_days = (train_df['date'].max() - train_df['date'].min()).days + 1\n",
        "expected_records = total_days * len(train_df['store_nbr'].unique()) * len(train_df['family'].unique())\n",
        "actual_records = len(train_df)\n",
        "print(f\"\\nTraining Data:\")\n",
        "print(f\"Expected Records: {expected_records:,}\")\n",
        "print(f\"Actual Records: {actual_records:,}\")\n",
        "print(f\"Completeness: {(actual_records/expected_records)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Checks\n",
        "\n",
        "def check_data_quality(df, date_col='date'):\n",
        "    \"\"\"\n",
        "    Perform various data quality checks on the dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\nData Quality Checks:\")\n",
        "    \n",
        "    # 1. Check for duplicates\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"\\n1. Duplicate Records: {duplicates:,} ({(duplicates/len(df))*100:.2f}%)\")\n",
        "    \n",
        "    # 2. Check for negative values in numeric columns\n",
        "    print(\"\\n2. Negative Values Check:\")\n",
        "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    for col in numeric_cols:\n",
        "        neg_count = (df[col] < 0).sum()\n",
        "        if neg_count > 0:\n",
        "            print(f\"   - {col}: {neg_count:,} negative values\")\n",
        "    \n",
        "    # 3. Check for date continuity if date column exists\n",
        "    if date_col in df.columns:\n",
        "        print(\"\\n3. Date Continuity Check:\")\n",
        "        dates = pd.date_range(start=df[date_col].min(), end=df[date_col].max())\n",
        "        missing_dates = set(dates) - set(df[date_col].unique())\n",
        "        if missing_dates:\n",
        "            print(f\"   - Missing dates: {len(missing_dates)}\")\n",
        "            print(f\"   - First few missing dates: {sorted(list(missing_dates))[:5]}\")\n",
        "        else:\n",
        "            print(\"   - No missing dates found\")\n",
        "    \n",
        "    # 4. Check for outliers in numeric columns using IQR method\n",
        "    print(\"\\n4. Outlier Detection (IQR method):\")\n",
        "    for col in numeric_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
        "        if outliers > 0:\n",
        "            print(f\"   - {col}: {outliers:,} outliers ({(outliers/len(df))*100:.2f}%)\")\n",
        "\n",
        "# Run quality checks on each dataset\n",
        "print(\"Quality Check - Training Data\")\n",
        "check_data_quality(train_df)\n",
        "\n",
        "print(\"\\nQuality Check - Oil Prices Data\")\n",
        "check_data_quality(oil_df)\n",
        "\n",
        "print(\"\\nQuality Check - Transactions Data\")\n",
        "check_data_quality(transactions_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Summary of Initial Data Analysis\n",
        "\n",
        "1. **Dataset Sizes and Structure**\n",
        "   - Training data spans multiple years with daily records\n",
        "   - Each record represents sales for a specific product family at a specific store\n",
        "   - Multiple supplementary datasets provide context (stores, oil prices, holidays)\n",
        "\n",
        "2. **Key Metrics**\n",
        "   - Number of stores\n",
        "   - Number of product families\n",
        "   - Date range coverage\n",
        "   - Transaction volumes\n",
        "   - Sales distributions\n",
        "\n",
        "3. **Data Quality Findings**\n",
        "   - Completeness of records\n",
        "   - Presence of outliers\n",
        "   - Missing value patterns\n",
        "   - Date continuity\n",
        "   - Data type consistency\n",
        "\n",
        "4. **Potential Challenges**\n",
        "   - Handling missing values\n",
        "   - Dealing with outliers\n",
        "   - Date alignment across datasets\n",
        "   - Memory optimization opportunities\n",
        "\n",
        "5. **Next Steps**\n",
        "   - Detailed temporal analysis\n",
        "   - Store-level analysis\n",
        "   - Product family analysis\n",
        "   - External factors correlation study\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all datasets\n",
        "train_df = pd.read_csv('../dataset/store-sales-time-series-forecasting/train.csv')\n",
        "stores_df = pd.read_csv('../dataset/store-sales-time-series-forecasting/stores.csv')\n",
        "oil_df = pd.read_csv('../dataset/store-sales-time-series-forecasting/oil.csv')\n",
        "holidays_df = pd.read_csv('../dataset/store-sales-time-series-forecasting/holidays_events.csv')\n",
        "transactions_df = pd.read_csv('../dataset/store-sales-time-series-forecasting/transactions.csv')\n",
        "\n",
        "# Convert date columns\n",
        "train_df['date'] = pd.to_datetime(train_df['date'])\n",
        "oil_df['date'] = pd.to_datetime(oil_df['date'])\n",
        "holidays_df['date'] = pd.to_datetime(holidays_df['date'])\n",
        "transactions_df['date'] = pd.to_datetime(transactions_df['date'])\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
